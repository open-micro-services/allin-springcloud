## ====服务端设置配置====
server:
    port: 8090
#
## ====Spark 数据解析流引擎配置==https://my.oschina.net/woter/blog/1843755==
## ====Spark SQL 方式HiveSql https://www.cnblogs.com/cord/p/9530404.html

# 打成jar包放到centos中执行
#https://blog.csdn.net/qq_41587243/article/details/112918052
spark:
    app:
        name: spark
    home: 192.168.0.105
    master:
        uri: spark://localhost:7077 #//local 本地开发模式
    driver:
        memory: 1g
        maxResultSize: 2g
    worker:
        memory: 1g
    executor:
        cores: 2
        memory: 1g
        # java.lang.IllegalArgumentException: requirement failed: spark.executor.heartbeatInterval should be less than or equal to spark.storage.blockManagerSlaveTimeoutMs
        heartbeatInterval: 1000000
    num:
        executors: 4
    network:
        timeout: 1474830
    rpc:
        message:
            maxSize: 1024   #默认值是128MB,最大2047MB
            #输序列化数据应该是有大小的限制，此错误消息意味着将一些较大的对象从driver端发送到executors
            #Job aborted due to stage failure: Serialized task 5829:0 was 354127887 bytes, which exceeds max allowed:spark.rpc.message.maxSize (134217728 bytes)
    storage:
        # java.lang.IllegalArgumentException: requirement failed: spark.executor.heartbeatInterval should be less than or equal to spark.storage.blockManagerSlaveTimeoutMs
        blockManagerSlaveTimeoutMs: 1000000
    jars: /usr/local/spark-3.0.0-bin-hadoop3.2/sbin/sb-spark-0.0.1-SNAPSHOT.jar #Spark SerializedLambda错误解决方案